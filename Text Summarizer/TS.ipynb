{
  "cells": [
    {
      "metadata": {
        "id": "8a77807f92f26ee"
      },
      "cell_type": "markdown",
      "source": [
        "# This is a Jupyter Notebook\n"
      ],
      "id": "8a77807f92f26ee"
    },
    {
      "metadata": {
        "id": "ed77764fdcf7586e"
      },
      "cell_type": "markdown",
      "source": [
        "Installs"
      ],
      "id": "ed77764fdcf7586e"
    },
    {
      "metadata": {
        "id": "fbc121e30a2defb3",
        "ExecuteTime": {
          "end_time": "2025-10-11T15:03:37.817166Z",
          "start_time": "2025-10-11T15:03:29.744231Z"
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U datasets --quiet\n",
        "!pip install ipywidgets --quiet\n",
        "!pip install py7zr --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install rouge-score --quiet\n",
        "!pip install hf_xet --quiet\n",
        "!pip install -U datasets transformers torch rouge-score --quiet\n",
        "!pip install bitsandbytes accelerate peft --quiet"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "eecdfc885ec3c643"
      },
      "cell_type": "markdown",
      "source": [
        "Importing dataset"
      ],
      "id": "eecdfc885ec3c643"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbdadbe471188b2c",
        "outputId": "8581efb4-eea6-4ad7-db69-02b4a9707dd7",
        "ExecuteTime": {
          "end_time": "2025-10-11T15:03:50.972266Z",
          "start_time": "2025-10-11T15:03:37.820682Z"
        }
      },
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "# Take just a small subset for quick training\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "small_eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(200))\n",
        "\n",
        "train_dataset = small_train_dataset"
      ],
      "id": "cbdadbe471188b2c",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a5a7bd34bbe1db",
        "outputId": "c0570b84-7a53-4658-b0a3-a777d0407e31",
        "ExecuteTime": {
          "end_time": "2025-10-11T15:03:50.993136Z",
          "start_time": "2025-10-11T15:03:50.974079Z"
        }
      },
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "id": "11a5a7bd34bbe1db",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 287113\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 13368\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 11490\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "c68742b95c1d1664"
      },
      "cell_type": "markdown",
      "source": [
        "Tokenizer"
      ],
      "id": "c68742b95c1d1664"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0310ba411f9457f",
        "outputId": "38483cb4-1118-4555-8a70-4982545550df",
        "ExecuteTime": {
          "end_time": "2025-10-11T15:04:03.132266Z",
          "start_time": "2025-10-11T15:03:51.012007Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig, default_data_collator\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def run_optimized_summarization():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_id = \"t5-small\"\n",
        "\n",
        "    MAX_ARTICLE_LENGTH = 768\n",
        "    MAX_SUMMARY_LENGTH = 128\n",
        "    TRAIN_SIZE = 500\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    print(\"Loading T5-small model with 4-bit Quantization...\")\n",
        "    try:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    except Exception:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
        "        print(\"Warning: Quantization failed. Loading full model.\")\n",
        "\n",
        "    raw_datasets = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "    raw_train = raw_datasets[\"train\"].shuffle(seed=42).select(range(TRAIN_SIZE))\n",
        "    raw_validation = raw_datasets[\"validation\"].shuffle(seed=42).select(range(50))\n",
        "\n",
        "    raw_small_datasets = DatasetDict({\"train\": raw_train, \"validation\": raw_validation})\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        inputs = [f\"summarize: {article}\" for article in examples[\"article\"]]\n",
        "\n",
        "        model_inputs = tokenizer(\n",
        "            inputs, max_length=MAX_ARTICLE_LENGTH, truncation=True, padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(\n",
        "                examples[\"highlights\"], max_length=MAX_SUMMARY_LENGTH, truncation=True, padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_datasets = raw_small_datasets.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"article\", \"highlights\", \"id\"],\n",
        "        num_proc=1\n",
        "    )\n",
        "\n",
        "    small_tokenized_train_dataset = tokenized_datasets[\"train\"]\n",
        "    small_tokenized_eval_dataset = tokenized_datasets[\"validation\"]\n",
        "\n",
        "    print(f\"Train Dataset size: {len(small_tokenized_train_dataset)} examples\")\n",
        "    print(f\"Validation Dataset size: {len(small_tokenized_eval_dataset)} examples\")\n",
        "\n",
        "    del raw_datasets, raw_train, raw_validation, raw_small_datasets, tokenized_datasets\n",
        "    gc.collect()\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        small_tokenized_train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=default_data_collator\n",
        "    )\n",
        "\n",
        "    print(\"\\nData is ready for training.\")\n",
        "    return model, tokenizer, small_tokenized_train_dataset, train_dataloader\n",
        "\n",
        "model, tokenizer, small_tokenized_train_dataset, train_dataloader = run_optimized_summarization()\n",
        "\n",
        "print(f\"\\nDataLoader created with {len(train_dataloader)} batches.\")"
      ],
      "id": "b0310ba411f9457f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading T5-small model with 4-bit Quantization...\n",
            "Tokenizing dataset...\n",
            "Train Dataset size: 500 examples\n",
            "Validation Dataset size: 50 examples\n",
            "\n",
            "Data is ready for training.\n",
            "\n",
            "DataLoader created with 125 batches.\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "7a5129e18dea9c74"
      },
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "id": "7a5129e18dea9c74"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba775576eaa50fcc",
        "outputId": "d95fb104-b803-4ed0-dc8c-7f2d0ce87609",
        "ExecuteTime": {
          "end_time": "2025-10-11T15:04:03.196050Z",
          "start_time": "2025-10-11T15:04:03.190506Z"
        }
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "train_dataset = small_tokenized_train_dataset\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=default_data_collator\n",
        ")\n",
        "\n",
        "print(f\"DataLoader created with {len(train_dataloader)} batches.\")"
      ],
      "id": "ba775576eaa50fcc",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader created with 125 batches.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "508c59334cc368ba"
      },
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "id": "508c59334cc368ba"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Diagnostic Check\n",
        "print(f\"Device being used: {device}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "print(f\"Starting training on {len(train_dataloader)} batches ({len(train_dataset)} examples).\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Use enumerate for tracking progress\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Select only the three necessary keys and move to device\n",
        "        inputs = {\n",
        "            \"input_ids\": batch[\"input_ids\"].to(device),\n",
        "            \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
        "            \"labels\": batch[\"labels\"].to(device)\n",
        "        }\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print loss update every 100 steps\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(f\"  Epoch {epoch+1}/{num_epochs}, Step {step+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} finished. Average Loss: {avg_loss:.4f} ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHBoekXI-Ufu",
        "outputId": "9d2131ea-2b7b-4a0d-847a-917cbaa45649",
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-10-11T15:04:03.208873Z"
        }
      },
      "id": "ZHBoekXI-Ufu",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device being used: cuda\n",
            "Starting training on 125 batches (500 examples).\n",
            "  Epoch 1/3, Step 100/125, Loss: nan\n",
            "\n",
            "--- Epoch 1/3 finished. Average Loss: nan ---\n",
            "  Epoch 2/3, Step 100/125, Loss: nan\n",
            "\n",
            "--- Epoch 2/3 finished. Average Loss: nan ---\n",
            "  Epoch 3/3, Step 100/125, Loss: nan\n",
            "\n",
            "--- Epoch 3/3 finished. Average Loss: nan ---\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "bd3a70d0972b97a2"
      },
      "cell_type": "markdown",
      "source": [
        "Generate Summary"
      ],
      "id": "bd3a70d0972b97a2"
    },
    {
      "metadata": {
        "id": "589d8400c0bf6b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f157f0ec-c431-46f4-e71d-2418cf83f1b0"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Article ---\n",
            "The annual Galactic Peace Summit, held this year on the orbital station Serenity Prime,\n",
            "concluded late Tuesday evening with a landmark agreement on interstellar trade routes.\n",
            "Representatives from the Federation of Planets and the independent merchant guilds spent\n",
            "three days locked in intense negotiations. The main point of contention was the taxation\n",
            "of highly valued crystalline resources mined from the Proxima Centauri system's asteroid belt.\n",
            "The final treaty establishes a zero-tariff policy for all humanitarian aid shipments and a\n",
            "graduated tax system for commercial goods, which is expected to boost cross-sector economic\n",
            "activity by 25% in the next fiscal cycle. Dr. Elara Vance, lead negotiator for the Federation,\n",
            "praised the outcome, calling it \"a new era of cooperation and shared prosperity.\" The agreement\n",
            "is set to be ratified by all major planetary councils within the next two weeks and will take\n",
            "effect immediately thereafter. Security forces reported no incidents, making this one of the\n",
            "smoothest summits in history.\n",
            "\n",
            "--- Generated Summary ---\n",
            "s\n"
          ]
        }
      ],
      "execution_count": 7,
      "source": [
        "def generate_summary(article):\n",
        "    # Prepare the input article\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        min_length=50,\n",
        "        max_length=200,\n",
        "\n",
        "        num_beams=4,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Testing\n",
        "sample_article = \"\"\"\n",
        "The annual Galactic Peace Summit, held this year on the orbital station Serenity Prime,\n",
        "concluded late Tuesday evening with a landmark agreement on interstellar trade routes.\n",
        "Representatives from the Federation of Planets and the independent merchant guilds spent\n",
        "three days locked in intense negotiations. The main point of contention was the taxation\n",
        "of highly valued crystalline resources mined from the Proxima Centauri system's asteroid belt.\n",
        "The final treaty establishes a zero-tariff policy for all humanitarian aid shipments and a\n",
        "graduated tax system for commercial goods, which is expected to boost cross-sector economic\n",
        "activity by 25% in the next fiscal cycle. Dr. Elara Vance, lead negotiator for the Federation,\n",
        "praised the outcome, calling it \"a new era of cooperation and shared prosperity.\" The agreement\n",
        "is set to be ratified by all major planetary councils within the next two weeks and will take\n",
        "effect immediately thereafter. Security forces reported no incidents, making this one of the\n",
        "smoothest summits in history.\n",
        "\"\"\"\n",
        "\n",
        "summary = generate_summary(sample_article)\n",
        "print(f\"--- Article ---\")\n",
        "print(f\"{sample_article.strip()}\")\n",
        "print(f\"\\n--- Generated Summary ---\")\n",
        "print(f\"{summary}\")"
      ],
      "id": "589d8400c0bf6b69"
    },
    {
      "metadata": {
        "id": "5ea4df0004724f07"
      },
      "cell_type": "markdown",
      "source": [
        "Rouge score"
      ],
      "id": "5ea4df0004724f07"
    },
    {
      "metadata": {
        "id": "659a2586c6f2ee88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e97cb9-8a62-4563-c7ee-a43e7db4d4ab"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
          ]
        }
      ],
      "execution_count": 8,
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def evaluate_summary(reference, generated):\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return scores\n",
        "\n",
        "# Example evaluation\n",
        "reference_summary = dataset['test'][0]['highlights']\n",
        "generated_summary = generate_summary(sample_article)\n",
        "\n",
        "scores = evaluate_summary(reference_summary, generated_summary)\n",
        "print(scores)"
      ],
      "id": "659a2586c6f2ee88"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}