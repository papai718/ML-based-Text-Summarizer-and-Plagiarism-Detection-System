{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {
    "id": "8a77807f92f26ee"
   },
   "source": [
    "# This is a Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77764fdcf7586e",
   "metadata": {
    "id": "ed77764fdcf7586e"
   },
   "source": [
    "Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:03:37.817166Z",
     "start_time": "2025-10-11T15:03:29.744231Z"
    },
    "id": "fbc121e30a2defb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets --quiet\n",
    "!pip install ipywidgets --quiet\n",
    "!pip install py7zr --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install torch --quiet\n",
    "!pip install rouge-score --quiet\n",
    "!pip install hf_xet --quiet\n",
    "!pip install -U datasets transformers torch rouge-score --quiet\n",
    "!pip install bitsandbytes accelerate peft --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecdfc885ec3c643",
   "metadata": {
    "id": "eecdfc885ec3c643"
   },
   "source": [
    "Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdadbe471188b2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:03:50.972266Z",
     "start_time": "2025-10-11T15:03:37.820682Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbdadbe471188b2c",
    "outputId": "8581efb4-eea6-4ad7-db69-02b4a9707dd7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "# Taking a small subset for quick training\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "small_eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "train_dataset = small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a5a7bd34bbe1db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:03:50.993136Z",
     "start_time": "2025-10-11T15:03:50.974079Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11a5a7bd34bbe1db",
    "outputId": "c0570b84-7a53-4658-b0a3-a777d0407e31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68742b95c1d1664",
   "metadata": {
    "id": "c68742b95c1d1664"
   },
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0310ba411f9457f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:04:03.132266Z",
     "start_time": "2025-10-11T15:03:51.012007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0310ba411f9457f",
    "outputId": "38483cb4-1118-4555-8a70-4982545550df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5-small model with 4-bit Quantization...\n",
      "Tokenizing dataset...\n",
      "Train Dataset size: 500 examples\n",
      "Validation Dataset size: 50 examples\n",
      "\n",
      "Data is ready for training.\n",
      "\n",
      "DataLoader created with 125 batches.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig, default_data_collator\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def run_optimized_summarization():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_id = \"t5-small\"\n",
    "\n",
    "    MAX_ARTICLE_LENGTH = 768\n",
    "    MAX_SUMMARY_LENGTH = 128\n",
    "    TRAIN_SIZE = 500\n",
    "    BATCH_SIZE = 4\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    print(\"Loading T5-small model with 4-bit Quantization...\")\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    except Exception:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "        print(\"Warning: Quantization failed. Loading full model.\")\n",
    "\n",
    "    raw_datasets = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "    raw_train = raw_datasets[\"train\"].shuffle(seed=42).select(range(TRAIN_SIZE))\n",
    "    raw_validation = raw_datasets[\"validation\"].shuffle(seed=42).select(range(50))\n",
    "\n",
    "    raw_small_datasets = DatasetDict({\"train\": raw_train, \"validation\": raw_validation})\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [f\"summarize: {article}\" for article in examples[\"article\"]]\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, max_length=MAX_ARTICLE_LENGTH, truncation=True, padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples[\"highlights\"], max_length=MAX_SUMMARY_LENGTH, truncation=True, padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_datasets = raw_small_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"article\", \"highlights\", \"id\"],\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    small_tokenized_train_dataset = tokenized_datasets[\"train\"]\n",
    "    small_tokenized_eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "    print(f\"Train Dataset size: {len(small_tokenized_train_dataset)} examples\")\n",
    "    print(f\"Validation Dataset size: {len(small_tokenized_eval_dataset)} examples\")\n",
    "\n",
    "    del raw_datasets, raw_train, raw_validation, raw_small_datasets, tokenized_datasets\n",
    "    gc.collect()\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        small_tokenized_train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator\n",
    "    )\n",
    "\n",
    "    print(\"\\nData is ready for training.\")\n",
    "    return model, tokenizer, small_tokenized_train_dataset, train_dataloader\n",
    "\n",
    "model, tokenizer, small_tokenized_train_dataset, train_dataloader = run_optimized_summarization()\n",
    "\n",
    "print(f\"\\nDataLoader created with {len(train_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5129e18dea9c74",
   "metadata": {
    "id": "7a5129e18dea9c74"
   },
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba775576eaa50fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:04:03.196050Z",
     "start_time": "2025-10-11T15:04:03.190506Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba775576eaa50fcc",
    "outputId": "d95fb104-b803-4ed0-dc8c-7f2d0ce87609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with 125 batches.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataset = small_tokenized_train_dataset\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(train_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c59334cc368ba",
   "metadata": {
    "id": "508c59334cc368ba"
   },
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ZHBoekXI-Ufu",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-11T15:04:03.208873Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHBoekXI-Ufu",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "9d2131ea-2b7b-4a0d-847a-917cbaa45649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n",
      "Starting training on 125 batches (500 examples).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\SUBHANKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SUBHANKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SUBHANKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\SUBHANKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\function.py:296\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Diagnostic Check\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "print(f\"Starting training on {len(train_dataloader)} batches ({len(train_dataset)} examples).\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Using enumerate for tracking progress\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            \"labels\": batch[\"labels\"].to(device)\n",
    "        }\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Printing loss update every 100 steps\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}, Step {step+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} finished. Average Loss: {avg_loss:.4f} ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a70d0972b97a2",
   "metadata": {
    "id": "bd3a70d0972b97a2"
   },
   "source": [
    "Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589d8400c0bf6b69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "589d8400c0bf6b69",
    "outputId": "f157f0ec-c431-46f4-e71d-2418cf83f1b0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[0;32m     20\u001b[0m sample_article \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124mThe annual Galactic Peace Summit, held this year on the orbital station Serenity Prime,\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124mconcluded late Tuesday evening with a landmark agreement on interstellar trade routes.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124msmoothest summits in history.\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_article\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Article ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_article\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[1;34m(article)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_summary\u001b[39m(article):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Preparing the input article\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(article, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m     summary_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m      7\u001b[0m         inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      8\u001b[0m         min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_summary(article):\n",
    "    # Preparing the input article\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        min_length=50,\n",
    "        max_length=200,\n",
    "\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Testing\n",
    "sample_article = \"\"\"\n",
    "The annual Galactic Peace Summit, held this year on the orbital station Serenity Prime,\n",
    "concluded late Tuesday evening with a landmark agreement on interstellar trade routes.\n",
    "Representatives from the Federation of Planets and the independent merchant guilds spent\n",
    "three days locked in intense negotiations. The main point of contention was the taxation\n",
    "of highly valued crystalline resources mined from the Proxima Centauri system's asteroid belt.\n",
    "The final treaty establishes a zero-tariff policy for all humanitarian aid shipments and a\n",
    "graduated tax system for commercial goods, which is expected to boost cross-sector economic\n",
    "activity by 25% in the next fiscal cycle. Dr. Elara Vance, lead negotiator for the Federation,\n",
    "praised the outcome, calling it \"a new era of cooperation and shared prosperity.\" The agreement\n",
    "is set to be ratified by all major planetary councils within the next two weeks and will take\n",
    "effect immediately thereafter. Security forces reported no incidents, making this one of the\n",
    "smoothest summits in history.\n",
    "\"\"\"\n",
    "\n",
    "summary = generate_summary(sample_article)\n",
    "print(f\"--- Article ---\")\n",
    "print(f\"{sample_article.strip()}\")\n",
    "print(f\"\\n--- Generated Summary ---\")\n",
    "print(f\"{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4df0004724f07",
   "metadata": {
    "id": "5ea4df0004724f07"
   },
   "source": [
    "Rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659a2586c6f2ee88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "659a2586c6f2ee88",
    "outputId": "81e97cb9-8a62-4563-c7ee-a43e7db4d4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def evaluate_summary(reference, generated):\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores\n",
    "\n",
    "# Example evaluation\n",
    "reference_summary = dataset['test'][0]['highlights']\n",
    "generated_summary = generate_summary(sample_article)\n",
    "\n",
    "scores = evaluate_summary(reference_summary, generated_summary)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
