{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "f81470a326610dc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.415772Z",
     "start_time": "2025-10-14T17:04:22.409267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset",
   "id": "ee8ffdd8ecfac84d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.443402Z",
     "start_time": "2025-10-14T17:04:22.431400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "dataset.head()"
   ],
   "id": "76cb74007a67030c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0                                        source_text  \\\n",
       "0           0  Researchers have discovered a new species of b...   \n",
       "1           1  The moon orbits the Earth in approximately 27....   \n",
       "2           2  Water is composed of two hydrogen atoms and on...   \n",
       "3           3          The history of Rome dates back to 753 BC.   \n",
       "4           4  Pluto was once considered the ninth planet in ...   \n",
       "\n",
       "                                    plagiarized_text  label  \n",
       "0  Scientists have found a previously unknown but...      1  \n",
       "1  Our natural satellite takes around 27.3 days t...      1  \n",
       "2  H2O consists of 2 hydrogen atoms and 1 oxygen ...      1  \n",
       "3  Rome has a long history that can be traced bac...      1  \n",
       "4  In the past, Pluto was classified as the ninth...      1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source_text</th>\n",
       "      <th>plagiarized_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Researchers have discovered a new species of b...</td>\n",
       "      <td>Scientists have found a previously unknown but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The moon orbits the Earth in approximately 27....</td>\n",
       "      <td>Our natural satellite takes around 27.3 days t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Water is composed of two hydrogen atoms and on...</td>\n",
       "      <td>H2O consists of 2 hydrogen atoms and 1 oxygen ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The history of Rome dates back to 753 BC.</td>\n",
       "      <td>Rome has a long history that can be traced bac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Pluto was once considered the ninth planet in ...</td>\n",
       "      <td>In the past, Pluto was classified as the ninth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.467929Z",
     "start_time": "2025-10-14T17:04:22.461086Z"
    }
   },
   "cell_type": "code",
   "source": "dataset['label'].value_counts()",
   "id": "f76e5cc0f1b6ba37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    187\n",
       "1    183\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Seperating into Training and Test Dataset",
   "id": "9e306c1150eb2ad5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.500503Z",
     "start_time": "2025-10-14T17:04:22.496985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "9816d195017a2761",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.514450Z",
     "start_time": "2025-10-14T17:04:22.509388Z"
    }
   },
   "cell_type": "code",
   "source": "print(y_train)",
   "id": "5afc8007d096611a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0\n",
      " 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1\n",
      " 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0\n",
      " 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
      " 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.535846Z",
     "start_time": "2025-10-14T17:04:22.532119Z"
    }
   },
   "cell_type": "code",
   "source": "print(y_test)",
   "id": "415408cf26e5435d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1\n",
      " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vectorization",
   "id": "3323a73e2fa8847c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.562937Z",
     "start_time": "2025-10-14T17:04:22.552216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(dataset[\"source_text\"] + \" \" + dataset[\"plagiarized_text\"])"
   ],
   "id": "adfd77cfb1277bae",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Applying logistic regression",
   "id": "3cbb15640161a873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.602684Z",
     "start_time": "2025-10-14T17:04:22.573106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "data[\"combined_text\"] = data[\"source_text\"] + \" \" + data[\"plagiarized_text\"]\n",
    "\n",
    "X_raw = data[\"combined_text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ],
   "id": "3990163722ad7bb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8648648648648649\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        35\n",
      "           1       0.87      0.87      0.87        39\n",
      "\n",
      "    accuracy                           0.86        74\n",
      "   macro avg       0.86      0.86      0.86        74\n",
      "weighted avg       0.86      0.86      0.86        74\n",
      "\n",
      "Confusion Matrix:\n",
      "[[30  5]\n",
      " [ 5 34]]\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Random Forest Model",
   "id": "9726aeb463837560"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.790489Z",
     "start_time": "2025-10-14T17:04:22.610880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ],
   "id": "3aad945213a6d51f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8243243243243243\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.81        35\n",
      "           1       0.81      0.87      0.84        39\n",
      "\n",
      "    accuracy                           0.82        74\n",
      "   macro avg       0.83      0.82      0.82        74\n",
      "weighted avg       0.83      0.82      0.82        74\n",
      "\n",
      "Confusion Matrix:\n",
      "[[27  8]\n",
      " [ 5 34]]\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Naiv Bays Model",
   "id": "391fe1a3383a821d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.812046Z",
     "start_time": "2025-10-14T17:04:22.797044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Generate classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ],
   "id": "51dbaa4db2d3ac5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8513513513513513\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.74      0.83        35\n",
      "           1       0.80      0.95      0.87        39\n",
      "\n",
      "    accuracy                           0.85        74\n",
      "   macro avg       0.87      0.85      0.85        74\n",
      "weighted avg       0.86      0.85      0.85        74\n",
      "\n",
      "Confusion Matrix:\n",
      "[[26  9]\n",
      " [ 2 37]]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SVM",
   "id": "54f844512b9b56c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.840346Z",
     "start_time": "2025-10-14T17:04:22.819303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate the model\n",
    "model = SVC(kernel='linear', random_state=42)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Generate classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ],
   "id": "fba8476c2d0815cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8648648648648649\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.84        35\n",
      "           1       0.82      0.95      0.88        39\n",
      "\n",
      "    accuracy                           0.86        74\n",
      "   macro avg       0.88      0.86      0.86        74\n",
      "weighted avg       0.87      0.86      0.86        74\n",
      "\n",
      "Confusion Matrix:\n",
      "[[27  8]\n",
      " [ 2 37]]\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.890540Z",
     "start_time": "2025-10-14T17:04:22.846258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate the model - FIX: Added probability=True\n",
    "model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Generate classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ],
   "id": "e6da34d7e4f0dd64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8648648648648649\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.84        35\n",
      "           1       0.82      0.95      0.88        39\n",
      "\n",
      "    accuracy                           0.86        74\n",
      "   macro avg       0.88      0.86      0.86        74\n",
      "weighted avg       0.87      0.86      0.86        74\n",
      "\n",
      "Confusion Matrix:\n",
      "[[27  8]\n",
      " [ 2 37]]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save SVM and Vectorizor",
   "id": "45120ad438b63958"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.901917Z",
     "start_time": "2025-10-14T17:04:22.895475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model,open(\"model.pkl\",'wb'))\n",
    "pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl','wb'))"
   ],
   "id": "4ac7e50019cd26ed",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Shingling",
   "id": "568543415a3ea1d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.918066Z",
     "start_time": "2025-10-14T17:04:22.913319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def shingling(text, k=3):\n",
    "    \"\"\"Generates k-shingles (word-grams) from a text.\"\"\"\n",
    "    words = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingles.add(' '.join(words[i:i+k]))\n",
    "    return shingles\n",
    "\n",
    "def jaccard_similarity(text1, text2, k=3):\n",
    "    \"\"\"Calculates Jaccard Similarity between two texts based on k-shingles.\"\"\"\n",
    "    shingles1 = shingling(text1, k)\n",
    "    shingles2 = shingling(text2, k)\n",
    "\n",
    "    # Jaccard = (Intersection) / (Union)\n",
    "    intersection = len(shingles1.intersection(shingles2))\n",
    "    union = len(shingles1.union(shingles2))\n",
    "\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union"
   ],
   "id": "18359a66dbc0b36e",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.929498Z",
     "start_time": "2025-10-14T17:04:22.924128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = dataset['source_text'].tolist() + dataset['plagiarized_text'].tolist()\n",
    "\n",
    "print(jaccard_similarity(texts[0], texts[len(dataset)]))"
   ],
   "id": "78768a12245ed863",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05263157894736842\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.949507Z",
     "start_time": "2025-10-14T17:04:22.944999Z"
    }
   },
   "cell_type": "code",
   "source": "print(jaccard_similarity(texts[0], texts[1]))",
   "id": "ab7b559b49ee39d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load Model and Vectorizer",
   "id": "d99f394763fc6aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:22.973459Z",
     "start_time": "2025-10-14T17:04:22.968607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = pickle.load(open('model.pkl','rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl','rb'))"
   ],
   "id": "4d8ea30a2cabe0eb",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:23.042532Z",
     "start_time": "2025-10-14T17:04:22.978988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "corpus = data[\"combined_text\"]\n",
    "\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=20000)\n",
    "X_char = tfidf_char.fit_transform(corpus)\n",
    "\n",
    "X_combined = hstack([X, X_char])"
   ],
   "id": "91e56b92268a362b",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Detection System",
   "id": "2788fd8fc9116299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:23.053430Z",
     "start_time": "2025-10-14T17:04:23.049261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect(input_text):\n",
    "    \"\"\"\n",
    "    Predicts plagiarism status for a given input text using the trained SVM model.\n",
    "    It simulates comparing the input text against a representative source text.\n",
    "    \"\"\"\n",
    "\n",
    "    source_text_example = dataset['source_text'][0]\n",
    "    combined_text = source_text_example + \" \" + input_text\n",
    "\n",
    "    X_input = tfidf_vectorizer.transform([combined_text])\n",
    "\n",
    "    prediction = model.predict(X_input)[0]\n",
    "\n",
    "    if prediction == 1:\n",
    "        return 'Plagiarism Detected'\n",
    "    else:\n",
    "        return 'No Plagiarism'"
   ],
   "id": "81877c26c0ad994f",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:23.123686Z",
     "start_time": "2025-10-14T17:04:23.058197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# FIX: Define 'corpus' using the combined text column from the 'data' DataFrame\n",
    "# 'data' was created in the Logistic Regression cell (Cell 8)\n",
    "corpus = data[\"combined_text\"]\n",
    "\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=20000)\n",
    "X_char = tfidf_char.fit_transform(corpus)\n",
    "\n",
    "# Note: X was already defined earlier in cell 7 and overwritten in cell 8\n",
    "# To ensure 'X' is correct for this step, we should re-extract it.\n",
    "# However, given the flow, we'll assume X is still correctly defined from the\n",
    "# Logistic Regression block where X = vectorizer.fit_transform(X_raw)\n",
    "\n",
    "X_combined = hstack([X, X_char])"
   ],
   "id": "ad32c7c629206714",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:23.136206Z",
     "start_time": "2025-10-14T17:04:23.129617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example ( it has no plagiarism)\n",
    "input_text = 'Playing musical instruments enhances creativity.'\n",
    "detect(input_text)"
   ],
   "id": "e97a46c1bc519f67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plagiarism Detected'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:23.164200Z",
     "start_time": "2025-10-14T17:04:23.156837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example ( it has no plagarism)\n",
    "input_text = 'Practicing yoga enhances physical flexibility.'\n",
    "detect(input_text)"
   ],
   "id": "4c7af2241e2f6b21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plagiarism Detected'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:04:23.182389Z",
     "start_time": "2025-10-14T17:04:23.176681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect(input_text):\n",
    "    \"\"\"\n",
    "    Predicts plagiarism status for a given input text using the trained SVM model.\n",
    "    It simulates comparing the input text against the 'source_text' from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    source_text_example = dataset['source_text'][0]\n",
    "    combined_text = source_text_example + \" \" + input_text\n",
    "\n",
    "    # Vectorize the combined text\n",
    "    X_input = tfidf_vectorizer.transform([combined_text])\n",
    "\n",
    "    # Predict the label (0: No Plagiarism, 1: Plagiarism Detected)\n",
    "    prediction = model.predict(X_input)[0]\n",
    "\n",
    "    if prediction == 1:\n",
    "        return 'Plagiarism Detected'\n",
    "    else:\n",
    "        return 'No Plagiarism'"
   ],
   "id": "f3487e4c36f4cf98",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SBERT",
   "id": "4f94f44e476d52e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:06:56.481300Z",
     "start_time": "2025-10-14T17:04:23.192218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install -U sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def chunk_text(text, chunk_size=80):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def semantic_similarity(doc1, doc2):\n",
    "    chunks1, chunks2 = chunk_text(doc1), chunk_text(doc2)\n",
    "    emb1, emb2 = sbert.encode(chunks1), sbert.encode(chunks2)\n",
    "    sims = cosine_similarity(emb1, emb2)\n",
    "    return float(np.max(sims))"
   ],
   "id": "40b6d820298b4815",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.8.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from sentence-transformers) (1.16.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arghy\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 564.3/564.3 kB 20.1 MB/s eta 0:00:00\n",
      "Downloading torch-2.8.0-cp312-cp312-win_amd64.whl (241.3 MB)\n",
      "   ---------------------------------------- 0.0/241.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 10.5/241.3 MB 50.4 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 11.5/241.3 MB 27.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 15.7/241.3 MB 25.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 17.8/241.3 MB 21.6 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 19.9/241.3 MB 19.4 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 21.8/241.3 MB 17.9 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 23.9/241.3 MB 16.6 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 26.0/241.3 MB 15.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 28.0/241.3 MB 15.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 29.9/241.3 MB 14.6 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 32.0/241.3 MB 14.2 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 34.1/241.3 MB 13.8 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 36.2/241.3 MB 13.4 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 37.5/241.3 MB 13.3 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 40.4/241.3 MB 13.0 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 42.2/241.3 MB 12.8 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 44.3/241.3 MB 12.6 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 46.4/241.3 MB 12.5 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 48.2/241.3 MB 12.4 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 50.3/241.3 MB 12.2 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 52.4/241.3 MB 12.1 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 54.5/241.3 MB 12.0 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 56.6/241.3 MB 11.9 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 58.7/241.3 MB 11.8 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 60.6/241.3 MB 11.8 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 62.7/241.3 MB 11.7 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 64.7/241.3 MB 11.6 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 66.8/241.3 MB 11.6 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 68.7/241.3 MB 11.5 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 70.8/241.3 MB 11.5 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 72.9/241.3 MB 11.4 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 75.0/241.3 MB 11.4 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 76.8/241.3 MB 11.3 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 78.9/241.3 MB 11.3 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 81.0/241.3 MB 11.2 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 83.1/241.3 MB 11.2 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 85.2/241.3 MB 11.2 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 87.0/241.3 MB 11.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 89.1/241.3 MB 11.1 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 91.2/241.3 MB 11.1 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 93.3/241.3 MB 11.0 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 95.4/241.3 MB 11.0 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 97.3/241.3 MB 11.0 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 99.4/241.3 MB 11.0 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 101.4/241.3 MB 10.9 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 103.5/241.3 MB 10.9 MB/s eta 0:00:13\n",
      "   ----------------- --------------------- 105.6/241.3 MB 10.9 MB/s eta 0:00:13\n",
      "   ----------------- --------------------- 107.7/241.3 MB 10.9 MB/s eta 0:00:13\n",
      "   ----------------- --------------------- 109.8/241.3 MB 10.9 MB/s eta 0:00:13\n",
      "   ------------------ -------------------- 111.7/241.3 MB 10.8 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 113.8/241.3 MB 10.8 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 115.9/241.3 MB 10.8 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 118.0/241.3 MB 10.8 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 119.8/241.3 MB 10.8 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 121.9/241.3 MB 10.8 MB/s eta 0:00:12\n",
      "   -------------------- ------------------ 124.0/241.3 MB 10.7 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 126.1/241.3 MB 10.7 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 128.2/241.3 MB 10.7 MB/s eta 0:00:11\n",
      "   --------------------- ----------------- 130.0/241.3 MB 10.7 MB/s eta 0:00:11\n",
      "   --------------------- ----------------- 132.1/241.3 MB 10.7 MB/s eta 0:00:11\n",
      "   --------------------- ----------------- 134.2/241.3 MB 10.7 MB/s eta 0:00:11\n",
      "   ---------------------- ---------------- 136.3/241.3 MB 10.7 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 138.4/241.3 MB 10.7 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 140.2/241.3 MB 10.6 MB/s eta 0:00:10\n",
      "   ----------------------- --------------- 142.3/241.3 MB 10.6 MB/s eta 0:00:10\n",
      "   ----------------------- --------------- 144.4/241.3 MB 10.6 MB/s eta 0:00:10\n",
      "   ----------------------- --------------- 146.3/241.3 MB 10.6 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 148.6/241.3 MB 10.6 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 150.7/241.3 MB 10.6 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 152.8/241.3 MB 10.6 MB/s eta 0:00:09\n",
      "   ------------------------- ------------- 154.9/241.3 MB 10.6 MB/s eta 0:00:09\n",
      "   ------------------------- ------------- 157.0/241.3 MB 10.6 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 158.6/241.3 MB 10.5 MB/s eta 0:00:08\n",
      "   -------------------------- ------------ 161.0/241.3 MB 10.5 MB/s eta 0:00:08\n",
      "   -------------------------- ------------ 163.1/241.3 MB 10.5 MB/s eta 0:00:08\n",
      "   -------------------------- ------------ 165.2/241.3 MB 10.5 MB/s eta 0:00:08\n",
      "   --------------------------- ----------- 167.2/241.3 MB 10.5 MB/s eta 0:00:08\n",
      "   --------------------------- ----------- 169.1/241.3 MB 10.5 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 171.2/241.3 MB 10.5 MB/s eta 0:00:07\n",
      "   ---------------------------- ---------- 173.3/241.3 MB 10.5 MB/s eta 0:00:07\n",
      "   ---------------------------- ---------- 175.4/241.3 MB 10.5 MB/s eta 0:00:07\n",
      "   ---------------------------- ---------- 177.5/241.3 MB 10.5 MB/s eta 0:00:07\n",
      "   ---------------------------- ---------- 179.3/241.3 MB 10.5 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 181.4/241.3 MB 10.5 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 183.5/241.3 MB 10.5 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 185.6/241.3 MB 10.5 MB/s eta 0:00:06\n",
      "   ------------------------------ -------- 187.4/241.3 MB 10.5 MB/s eta 0:00:06\n",
      "   ------------------------------ -------- 189.8/241.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 191.9/241.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 194.0/241.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 196.1/241.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 197.9/241.3 MB 10.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 200.0/241.3 MB 10.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 202.1/241.3 MB 10.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 204.2/241.3 MB 10.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 206.0/241.3 MB 10.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 208.1/241.3 MB 10.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 210.2/241.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 212.1/241.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 214.2/241.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 216.3/241.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 218.4/241.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 220.5/241.3 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 222.3/241.3 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 224.4/241.3 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 226.5/241.3 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 228.6/241.3 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 230.7/241.3 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 232.8/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 234.6/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  236.7/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  238.8/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  240.9/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 241.3/241.3 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.9/12.0 MB 21.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.2/12.0 MB 26.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 22.7 MB/s eta 0:00:00\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl (275 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 25.9 MB/s eta 0:00:00\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, regex, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.9.0 huggingface-hub-0.35.3 mpmath-1.3.0 networkx-3.5 regex-2025.9.18 safetensors-0.6.2 sentence-transformers-5.1.1 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 transformers-4.57.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arghy\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\arghy\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arghy\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
